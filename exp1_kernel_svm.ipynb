{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset prepareation\n",
    "First the dataset needs to be loaded and modified to only include the labels we want. This is done by merging unchosen labels to their closest related label. Cases where text has multiple labels for one entry are handled by choosing the label which has the lowest representation in the dataset so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.dataset import EmotionsDataset\n",
    "\n",
    "train_dataset = EmotionsDataset(split=\"train\")\n",
    "valid_dataset = EmotionsDataset(split=\"valid\")\n",
    "test_dataset = EmotionsDataset(split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation Methods\n",
    "## Confusion Matrix\n",
    "A confusion matrix is used to visualise the performance of the classifier, this helps us see which labels the classifiers are making the most mistakes on.\n",
    "The dataset is largely saturated by neutral tags, this means a raw confusion matrix turns out dark for all values apart from neural-neutral, to solve this my confusion matrix displays the log values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from data.dataset import label_names, chosen_labels\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def get_label_names():\n",
    "    names = []\n",
    "\n",
    "    for label in chosen_labels:\n",
    "        names.append(label_names[label])\n",
    "    names.append(label_names[27])\n",
    "\n",
    "    return names\n",
    "\n",
    "def display_confusion_matrix(preds, y, scale=\"linear\", save_name: Tuple(str, str) = None):\n",
    "    confusion_matrix = np.zeros((14,14)).astype(np.int32)\n",
    "\n",
    "    for pred, label in zip(preds, y):\n",
    "        confusion_matrix[pred, label] += 1\n",
    "\n",
    "    if scale == \"log\":\n",
    "        confusion_matrix = np.log2(confusion_matrix+1)\n",
    "    \n",
    "    chosen_label_names = get_label_names()\n",
    "    heatmap_confusion_matrix = pd.DataFrame(confusion_matrix, index=chosen_label_names, columns=chosen_label_names)\n",
    "\n",
    "    ax = plt.axes()\n",
    "    sb.heatmap(heatmap_confusion_matrix, annot=True, ax=ax)\n",
    "    scale_title = scale[0].upper() + scale[1:]\n",
    "    ax.set_title(f\"{scale_title} Confusion Matrix\")\n",
    "\n",
    "    if save_name != None:\n",
    "        plt.savefig(f\"./figures/{save_name[0]}/{save_name[1]}.png\")\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, F1 Score\n",
    "To keep track of the performance of each experiment on an induvidual label, by tracking how often a label is corrcetly guessed out of the the instances of that label and how ofter a guess of a label is correct out of all the guesses of that label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def get_scores(labels, predictions, kernel_type = \"linear\"):\n",
    "    prec, recall, f1, support = precision_recall_fscore_support(labels, predictions)\n",
    "\n",
    "    results_dictionary = {\"label\":list(range(14)), \"precision\":prec, \"recall\":recall, \"f1\":f1}\n",
    "    results = pd.DataFrame(results_dictionary)\n",
    "    results.to_csv(f\"figures/{kernel_type}_kernel_metrics.csv\", index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Different SVM kernel types\n",
    "This experiment looks at the different kernel options used to build a support vector, the kernels covered are: linear, polynomial, radial basis function and sigmoid.\n",
    "## Vectorisation using TFIDF \n",
    "The input text needs to be vectorised before it can be used for training a support vector machine. First stop words and non-alphabetic words are removed before being lemmatised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download, pos_tag\n",
    "download('punkt')\n",
    "download('wordnet')\n",
    "download('omw-1.4')\n",
    "download('averaged_perceptron_tagger')\n",
    "download('stopwords')\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def lem_text(text):\n",
    "    text = [word.lower() for word in text]\n",
    "    text = [word_tokenize(word) for word in text]\n",
    "\n",
    "    word_tags = defaultdict(lambda: wordnet.NOUN)\n",
    "    word_tags['J'] = wordnet.ADJ\n",
    "    word_tags['V'] = wordnet.VERB\n",
    "    word_tags['R'] = wordnet.ADV\n",
    "\n",
    "    for i, words in enumerate(text):\n",
    "        lemmed_text = []\n",
    "        lemmer = WordNetLemmatizer()\n",
    "\n",
    "        for word, tag in pos_tag(words):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                lem_word = lemmer.lemmatize(word, word_tags[tag[0]])\n",
    "                lemmed_text.append(lem_word)\n",
    "\n",
    "        text[i] = str(lemmed_text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the text of all datasets has been lemmatised, the training data is used to extract the most important word tfidf features. These features are used to convert all dataset text entries into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_dataset_tfidf = EmotionsDataset(split=\"train\")\n",
    "test_dataset_tfidf = EmotionsDataset(split=\"test\")\n",
    "\n",
    "train_dataset_tfidf.x = lem_text(train_dataset_tfidf.x)\n",
    "test_dataset_tfidf.x = lem_text(test_dataset_tfidf.x)\n",
    "\n",
    "tfidf_vectoriser = TfidfVectorizer(max_features=1000)\n",
    "tfidf_vectoriser.fit(train_dataset_tfidf.x)\n",
    "\n",
    "train_dataset_tfidf.x = tfidf_vectoriser.transform(train_dataset_tfidf.x)\n",
    "test_dataset_tfidf.x = tfidf_vectoriser.transform(test_dataset_tfidf.x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM kernel results\n",
    "The performance of the SVM is evaluated with accuracy, recall, precision and f1 score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "kernel_dir = \"kernel_experiement\"\n",
    "\n",
    "svm_linear = svm.SVC(kernel='linear')\n",
    "svm_linear.fit(train_dataset_tfidf.x, train_dataset_tfidf.y)\n",
    "preds = svm_linear.predict(test_dataset_tfidf.x)\n",
    "\n",
    "display_confusion_matrix(preds, test_dataset_tfidf.y, save_name=(kernel_dir, \"confusion_matrix_no_scaling\"))\n",
    "display_confusion_matrix(preds, test_dataset_tfidf.y, scale=\"log\", save_name=(kernel_dir, 'linear_matrix'))\n",
    "\n",
    "acc = accuracy_score(test_dataset_tfidf.y, preds)*100\n",
    "\n",
    "results = get_scores(test_dataset_tfidf.y, preds, kernel_type=\"linear\")\n",
    "print(results)\n",
    "print(\"Testing Accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = svm.SVC(kernel='poly')\n",
    "svm_poly.fit(train_dataset_tfidf.x, train_dataset_tfidf.y)\n",
    "preds = svm_poly.predict(test_dataset_tfidf.x)\n",
    "\n",
    "display_confusion_matrix(preds, test_dataset_tfidf.y, scale=\"log\", save_name=(kernel_dir, 'poly_matrix'))\n",
    "\n",
    "acc = accuracy_score(test_dataset_tfidf.y, preds)*100\n",
    "prec, recall, f1, support = precision_recall_fscore_support(test_dataset_tfidf.y, preds)\n",
    "\n",
    "results = get_scores(test_dataset_tfidf.y, preds, kernel_type=\"poly\")\n",
    "\n",
    "print(results)\n",
    "print(\"Testing Accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sig = svm.SVC(kernel='sigmoid')\n",
    "svm_sig.fit(train_dataset_tfidf.x, train_dataset_tfidf.y)\n",
    "preds = svm_sig.predict(test_dataset_tfidf.x)\n",
    "\n",
    "display_confusion_matrix(preds, test_dataset_tfidf.y, scale=\"log\", save_name=(kernel_dir, 'sigmoid_matrix'))\n",
    "\n",
    "acc = accuracy_score(test_dataset_tfidf.y, preds)*100\n",
    "\n",
    "results = get_scores(test_dataset_tfidf.y, preds, kernel_type=\"sigmoid\")\n",
    "\n",
    "print(results)\n",
    "print(\"Testing Accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = svm.SVC(kernel='rbf')\n",
    "svm_rbf.fit(train_dataset_tfidf.x, train_dataset_tfidf.y)\n",
    "preds = svm_rbf.predict(test_dataset_tfidf.x)\n",
    "\n",
    "display_confusion_matrix(preds, test_dataset_tfidf.y, scale=\"log\", save_name=(kernel_dir, 'rbf_matrix'))\n",
    "\n",
    "acc = accuracy_score(test_dataset_tfidf.y, preds)*100\n",
    "\n",
    "results = get_scores(test_dataset_tfidf.y, preds, kernel_type=\"rbf\")\n",
    "\n",
    "print(results)\n",
    "print(\"Testing Accuracy: \", acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: SVM Hyper-parameter tuning\n",
    "I will tune the hyper-parameters of SVM while keeping the kernel the same. I will use the radial basis function kernel as it has provided the best results in experiment 1.\n",
    "## Grid Search\n",
    "The hyper-parameters will be searched using a grid search technique. I will select a set of discrete values for each variable that I am changing. The variables I will focus on will be: the regularisation parameter, gamma value and stopping tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = [0.5, 0.75, 1, 1.25, 1.5]\n",
    "g_values = [0.5, 0.75, 1, 1.25, 1.5]\n",
    "t_values = [1e-2, 1e-3, 1e-4]\n",
    "\n",
    "grid_search_dir = 'grid_search_experiment'\n",
    "\n",
    "for c_value in c_values:\n",
    "    for g_value in g_values:\n",
    "        for t_value in t_values:\n",
    "            test_name = f'{c_value}_{g_value}_{t_value}'\n",
    "\n",
    "            svm_rbf = svm.SVC(kernel='rbf')\n",
    "            svm_rbf.fit(train_dataset_tfidf.x, train_dataset_tfidf.y)\n",
    "\n",
    "            preds = svm_rbf.predict(test_dataset_tfidf.x)\n",
    "\n",
    "            display_confusion_matrix(preds, test_dataset_tfidf.y, scale=\"log\", save_name=(grid_search_dir, f'{test_name}_matrix'))\n",
    "\n",
    "            acc = accuracy_score(test_dataset_tfidf.y, preds)*100\n",
    "\n",
    "            results = get_scores(test_dataset_tfidf.y, preds, kernel_type=\"rbf\")\n",
    "\n",
    "            print(results)\n",
    "            print(f\"Testing Accuracy ({test_name}): \", acc)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e90b2c26d89afd6ff4531a86385273d4d7b0d022c36c5286aaec0b90c4b93a04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
